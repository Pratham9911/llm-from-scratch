From Text to Final Embeddings 

This project shows how raw text is converted into GPT-ready embeddings step by step.

ðŸ”¹ Step 1: Raw Text

We start with plain text (a book, paragraph, etc.).

"Hello, do you like tea?"

ðŸ”¹ Step 2: Tokenization

The text is converted into token IDs using a tokenizer.

Text â†’ [10, 45, 78, 91]


Tokens are numbers.
Models cannot understand text directly.

ðŸ”¹ Step 3: Dataset (Sliding Window)

The token IDs are split into inputâ€“target pairs using a sliding window.

Example (max_length = 4):

Input  â†’ [10, 45, 78, 91]
Target â†’ [45, 78, 91, 33]


This trains the model to predict the next token.

ðŸ”¹ Step 4: DataLoader (Batching)

The DataLoader groups many inputâ€“target pairs into batches.

Example (batch_size = 2):

Inputs  â†’ shape (2, 4)
Targets â†’ shape (2, 4)


Batching makes training efficient.

ðŸ”¹ Step 5: Token Embeddings

Each token ID is converted into a vector using an embedding layer.

Token IDs â†’ Vectors


Shape becomes:

(batch_size, seq_length, embedding_dim)


Example:

(2, 4, 64)

ðŸ”¹ Step 6: Positional Embeddings

Since models donâ€™t know word order, we add position information.

One vector per position

Same positions shared across the batch

Position 0 â†’ vector
Position 1 â†’ vector
...

ðŸ”¹ Step 7: Final Embeddings

Token embeddings and positional embeddings are added together.

Final embedding = token embedding + positional embedding


Final shape (GPT input):

(batch_size, seq_length, embedding_dim)


Example:

torch.Size([8, 4, 256])

